"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook=globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook||[]).push([[974],{5244:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>p,frontMatter:()=>a,metadata:()=>o,toc:()=>c});var t=s(4848),i=s(8453);const a={},r="Chapter 2: Practical Applications of Sensory Systems",o={id:"textbook/chapter2/practical",title:"Chapter 2: Practical Applications of Sensory Systems",description:"Implementation of Vision Systems",source:"@site/docs/textbook/chapter2/practical.md",sourceDirName:"textbook/chapter2",slug:"/textbook/chapter2/practical",permalink:"/Ai-book/docs/textbook/chapter2/practical",draft:!1,unlisted:!1,editUrl:"https://github.com/AftabAhmed4/Ai-book/edit/main/docs/textbook/chapter2/practical.md",tags:[],version:"current",frontMatter:{},sidebar:"textbookSidebar",previous:{title:"Chapter 2: Theoretical Foundations of Sensory Systems",permalink:"/Ai-book/docs/textbook/chapter2/theory"},next:{title:"Chapter 2: Exercises and Problems",permalink:"/Ai-book/docs/textbook/chapter2/exercises"}},l={},c=[{value:"Implementation of Vision Systems",id:"implementation-of-vision-systems",level:2},{value:"Setting Up a Stereo Vision System",id:"setting-up-a-stereo-vision-system",level:3},{value:"Image Preprocessing Pipeline",id:"image-preprocessing-pipeline",level:3},{value:"Implementing Auditory Systems",id:"implementing-auditory-systems",level:2},{value:"Sound Capture and Processing",id:"sound-capture-and-processing",level:3},{value:"Feature Extraction for Recognition",id:"feature-extraction-for-recognition",level:3},{value:"Tactile Sensor Integration",id:"tactile-sensor-integration",level:2},{value:"Simulating Tactile Sensors",id:"simulating-tactile-sensors",level:3},{value:"Sensor Fusion Implementation",id:"sensor-fusion-implementation",level:2},{value:"Kalman Filter for Position Tracking",id:"kalman-filter-for-position-tracking",level:3},{value:"Real-time Processing Considerations",id:"real-time-processing-considerations",level:2},{value:"Optimized Sensor Processing Pipeline",id:"optimized-sensor-processing-pipeline",level:3},{value:"Integration with Robot Control",id:"integration-with-robot-control",level:2},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Profiling and Optimization Techniques",id:"profiling-and-optimization-techniques",level:3},{value:"Conclusion",id:"conclusion",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",p:"p",pre:"pre",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"chapter-2-practical-applications-of-sensory-systems",children:"Chapter 2: Practical Applications of Sensory Systems"}),"\n",(0,t.jsx)(n.h2,{id:"implementation-of-vision-systems",children:"Implementation of Vision Systems"}),"\n",(0,t.jsx)(n.p,{children:"In this section, we'll explore practical implementations of vision systems for humanoid robots using the OpenCV library and other computer vision tools."}),"\n",(0,t.jsx)(n.h3,{id:"setting-up-a-stereo-vision-system",children:"Setting Up a Stereo Vision System"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import cv2\nimport numpy as np\n\n# Initialize stereo cameras\nleft_cam = cv2.VideoCapture(0)  # Left camera\nright_cam = cv2.VideoCapture(1)  # Right camera\n\n# Camera calibration parameters (would be obtained from calibration)\nleft_camera_matrix = np.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]])\nright_camera_matrix = np.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]])\ndistortion_coeffs = np.zeros((5, 1))\n\ndef get_depth_map(left_img, right_img):\n    # Create stereo matcher\n    stereo = cv2.StereoSGBM_create(\n        minDisparity=0,\n        numDisparities=16*10,  # Must be divisible by 16\n        blockSize=15,\n        P1=8 * 3 * 15**2,\n        P2=32 * 3 * 15**2,\n        disp12MaxDiff=1,\n        uniquenessRatio=15,\n        speckleWindowSize=0,\n        speckleRange=2,\n        preFilterCap=63,\n        mode=cv2.STEREO_SGBM_MODE_SGBM_3WAY\n    )\n    \n    # Compute disparity map\n    disparity = stereo.compute(left_img, right_img).astype(np.float32)\n    disparity = disparity / 16.0  # Adjust for SGBM algorithm\n    \n    return disparity\n"})}),"\n",(0,t.jsx)(n.h3,{id:"image-preprocessing-pipeline",children:"Image Preprocessing Pipeline"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"def preprocess_image(image):\n    # Convert to grayscale if needed\n    if len(image.shape) == 3:\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    else:\n        gray = image\n    \n    # Apply Gaussian blur to reduce noise\n    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n    \n    # Enhance contrast using histogram equalization\n    equalized = cv2.equalizeHist(blurred)\n    \n    return equalized\n\ndef detect_features(image):\n    # Using ORB detector\n    orb = cv2.ORB_create()\n    keypoints, descriptors = orb.detectAndCompute(image, None)\n    \n    return keypoints, descriptors\n"})}),"\n",(0,t.jsx)(n.h2,{id:"implementing-auditory-systems",children:"Implementing Auditory Systems"}),"\n",(0,t.jsx)(n.h3,{id:"sound-capture-and-processing",children:"Sound Capture and Processing"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import pyaudio\nimport numpy as np\nfrom scipy import signal\nimport librosa\n\nclass AudioProcessor:\n    def __init__(self):\n        self.chunk = 1024\n        self.format = pyaudio.paInt16\n        self.channels = 1\n        self.rate = 44100\n        self.audio = pyaudio.PyAudio()\n        \n    def start_recording(self, duration=5):\n        # Open stream\n        stream = self.audio.open(\n            format=self.format,\n            channels=self.channels,\n            rate=self.rate,\n            input=True,\n            frames_per_buffer=self.chunk\n        )\n        \n        print(\"Recording...\")\n        frames = []\n        \n        for i in range(0, int(self.rate / self.chunk * duration)):\n            data = stream.read(self.chunk)\n            frames.append(data)\n        \n        print(\"Finished recording\")\n        \n        # Close stream\n        stream.stop_stream()\n        stream.close()\n        \n        # Convert to numpy array\n        audio_data = np.frombuffer(b''.join(frames), dtype=np.int16)\n        return audio_data.astype(np.float32) / 32768.0  # Normalize\n    \n    def apply_noise_reduction(self, audio_data):\n        # Apply a simple low-pass filter\n        b, a = signal.butter(4, 0.1, btype='low', fs=self.rate)\n        filtered_audio = signal.filtfilt(b, a, audio_data)\n        return filtered_audio\n    \n    def detect_sound_direction(self, audio_data_left, audio_data_right):\n        # Simple interaural time difference (ITD) estimation\n        correlation = np.correlate(audio_data_left, audio_data_right, mode='full')\n        lag = correlation.argmax() - (len(audio_data_right) - 1)\n        \n        # Convert lag to direction estimate\n        # This is a simplified model\n        max_lag = len(audio_data_left) - 1\n        direction = (lag / max_lag) * 180  # degrees\n        \n        return direction\n"})}),"\n",(0,t.jsx)(n.h3,{id:"feature-extraction-for-recognition",children:"Feature Extraction for Recognition"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"def extract_audio_features(audio_data, sr=44100):\n    # Extract MFCCs (Mel-frequency cepstral coefficients)\n    mfccs = librosa.feature.mfcc(y=audio_data, sr=sr, n_mfcc=13)\n    \n    # Extract spectral centroid (measure of brightness)\n    spectral_centroids = librosa.feature.spectral_centroid(y=audio_data, sr=sr)[0]\n    \n    # Extract spectral rolloff\n    spectral_rolloff = librosa.feature.spectral_rolloff(y=audio_data, sr=sr)[0]\n    \n    # Extract zero crossing rate\n    zcr = librosa.feature.zero_crossing_rate(audio_data)[0]\n    \n    # Extract chroma feature\n    chroma = librosa.feature.chroma_stft(y=audio_data, sr=sr)\n    \n    return {\n        'mfccs': mfccs,\n        'spectral_centroids': spectral_centroids,\n        'spectral_rolloff': spectral_rolloff,\n        'zcr': zcr,\n        'chroma': chroma\n    }\n"})}),"\n",(0,t.jsx)(n.h2,{id:"tactile-sensor-integration",children:"Tactile Sensor Integration"}),"\n",(0,t.jsx)(n.h3,{id:"simulating-tactile-sensors",children:"Simulating Tactile Sensors"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import numpy as np\nimport matplotlib.pyplot as plt\n\nclass TactileSensor:\n    def __init__(self, rows=16, cols=16, resolution=1.0):\n        self.rows = rows\n        self.cols = cols\n        self.resolution = resolution  # resolution in mm\n        self.pressure_map = np.zeros((rows, cols))\n        \n    def detect_contact(self, force_matrix):\n        """Simulate tactile sensor readings"""\n        # Apply some noise to simulate real sensor\n        noise = np.random.normal(0, 0.05, force_matrix.shape)\n        self.pressure_map = np.maximum(0, force_matrix + noise)\n        return self.pressure_map\n    \n    def extract_features(self):\n        """Extract features from pressure distribution"""\n        # Calculate center of pressure\n        y_indices, x_indices = np.ogrid[:self.rows, :self.cols]\n        total_force = np.sum(self.pressure_map)\n        \n        if total_force > 0:\n            center_y = np.sum(y_indices * self.pressure_map) / total_force\n            center_x = np.sum(x_indices * self.pressure_map) / total_force\n        else:\n            center_y, center_x = self.rows/2, self.cols/2\n            \n        # Calculate contact area (area with pressure above threshold)\n        contact_area = np.sum(self.pressure_map > 0.1)\n        \n        # Calculate pressure distribution metrics\n        mean_pressure = np.mean(self.pressure_map)\n        std_pressure = np.std(self.pressure_map)\n        \n        return {\n            \'center_of_pressure\': (center_x, center_y),\n            \'contact_area\': contact_area,\n            \'mean_pressure\': mean_pressure,\n            \'std_pressure\': std_pressure\n        }\n\n# Example usage\ntactile = TactileSensor()\n# Simulate force applied to the sensor area\nforce_matrix = np.zeros((16, 16))\nforce_matrix[5:10, 5:10] = 0.8  # Apply force to a 5x5 region\npressure_map = tactile.detect_contact(force_matrix)\nfeatures = tactile.extract_features()\n'})}),"\n",(0,t.jsx)(n.h2,{id:"sensor-fusion-implementation",children:"Sensor Fusion Implementation"}),"\n",(0,t.jsx)(n.h3,{id:"kalman-filter-for-position-tracking",children:"Kalman Filter for Position Tracking"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import numpy as np\n\nclass KalmanFilter:\n    def __init__(self, dt, measurement_noise=1, process_noise=1):\n        # State vector: [x, y, vx, vy] (position and velocity)\n        self.n = 4\n        self.m = 2  # measurement vector size [x, y]\n        \n        # Time step\n        self.dt = dt\n        \n        # State transition matrix\n        self.F = np.array([\n            [1, 0, dt, 0],\n            [0, 1, 0, dt],\n            [0, 0, 1, 0],\n            [0, 0, 0, 1]\n        ])\n        \n        # Measurement matrix\n        self.H = np.array([\n            [1, 0, 0, 0],\n            [0, 1, 0, 0]\n        ])\n        \n        # Process noise covariance\n        self.Q = process_noise * np.array([\n            [dt**4/4, 0, dt**3/2, 0],\n            [0, dt**4/4, 0, dt**3/2],\n            [dt**3/2, 0, dt**2, 0],\n            [0, dt**3/2, 0, dt**2]\n        ])\n        \n        # Measurement noise covariance\n        self.R = measurement_noise * np.eye(2)\n        \n        # Initial state covariance\n        self.P = np.eye(4) * 1000\n        \n        # Initial state\n        self.x = np.zeros(4)\n    \n    def predict(self):\n        """Predict next state"""\n        self.x = self.F @ self.x\n        self.P = self.F @ self.P @ self.F.T + self.Q\n        return self.x\n    \n    def update(self, measurement):\n        """Update state with new measurement"""\n        # Calculate innovation\n        y = measurement - self.H @ self.x\n        S = self.H @ self.P @ self.H.T + self.R\n        \n        # Calculate Kalman gain\n        K = self.P @ self.H.T @ np.linalg.inv(S)\n        \n        # Update state and covariance\n        self.x = self.x + K @ y\n        self.P = (np.eye(4) - K @ self.H) @ self.P\n        \n        return self.x\n\n# Example sensor fusion using Kalman filter\ndef sensor_fusion_example():\n    # Initialize Kalman filter\n    kf = KalmanFilter(dt=0.1, measurement_noise=0.1, process_noise=0.1)\n    \n    # Simulate measurements from different sensors\n    measurements = []\n    for i in range(100):\n        # Simulate true position with noise\n        true_pos = np.array([i*0.1 + np.random.normal(0, 0.05), \n                            0.5*np.sin(i*0.1) + np.random.normal(0, 0.05)])\n        measurements.append(true_pos)\n    \n    # Track with Kalman filter\n    filtered_states = []\n    for measurement in measurements:\n        kf.predict()\n        state = kf.update(measurement)\n        filtered_states.append(state.copy())\n    \n    return np.array(filtered_states), np.array(measurements)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"real-time-processing-considerations",children:"Real-time Processing Considerations"}),"\n",(0,t.jsx)(n.h3,{id:"optimized-sensor-processing-pipeline",children:"Optimized Sensor Processing Pipeline"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import threading\nimport queue\nimport time\n\nclass RealtimeSensorProcessor:\n    def __init__(self):\n        self.vision_queue = queue.Queue(maxsize=10)\n        self.audio_queue = queue.Queue(maxsize=10)\n        self.tactile_queue = queue.Queue(maxsize=10)\n        \n        self.running = False\n        self.vision_thread = None\n        self.audio_thread = None\n        self.fusion_thread = None\n        \n    def start_processing(self):\n        self.running = True\n        \n        # Start sensor processing threads\n        self.vision_thread = threading.Thread(target=self._process_vision)\n        self.audio_thread = threading.Thread(target=self._process_audio)\n        self.fusion_thread = threading.Thread(target=self._sensor_fusion)\n        \n        self.vision_thread.start()\n        self.audio_thread.start()\n        self.fusion_thread.start()\n        \n    def stop_processing(self):\n        self.running = False\n        \n        # Wait for threads to finish\n        if self.vision_thread:\n            self.vision_thread.join()\n        if self.audio_thread:\n            self.audio_thread.join()\n        if self.fusion_thread:\n            self.fusion_thread.join()\n    \n    def _process_vision(self):\n        """Continuously process vision data"""\n        cap = cv2.VideoCapture(0)\n        \n        while self.running:\n            ret, frame = cap.read()\n            if ret:\n                # Process frame\n                processed_frame = preprocess_image(frame)\n                keypoints, descriptors = detect_features(processed_frame)\n                \n                # Put vision data in queue for fusion\n                vision_data = {\n                    \'frame\': processed_frame,\n                    \'keypoints\': keypoints,\n                    \'timestamp\': time.time()\n                }\n                \n                if not self.vision_queue.full():\n                    self.vision_queue.put(vision_data)\n            \n            time.sleep(0.033)  # ~30 FPS\n        \n        cap.release()\n    \n    def _process_audio(self):\n        """Continuously process audio data"""\n        audio_proc = AudioProcessor()\n        \n        while self.running:\n            # Record a short snippet\n            audio_data = audio_proc.start_recording(duration=0.5)\n            features = extract_audio_features(audio_data)\n            \n            audio_data = {\n                \'features\': features,\n                \'timestamp\': time.time()\n            }\n            \n            if not self.audio_queue.full():\n                self.audio_queue.put(audio_data)\n            \n            time.sleep(0.1)  # Process every 100ms\n    \n    def _sensor_fusion(self):\n        """Fuse data from multiple sensors"""\n        while self.running:\n            # Get data from all queues\n            vision_data = None\n            audio_data = None\n            tactile_data = None\n            \n            # Non-blocking check for data\n            try:\n                vision_data = self.vision_queue.get_nowait()\n            except queue.Empty:\n                pass\n                \n            try:\n                audio_data = self.audio_queue.get_nowait()\n            except queue.Empty:\n                pass\n                \n            try:\n                tactile_data = self.tactile_queue.get_nowait()\n            except queue.Empty:\n                pass\n            \n            # Perform fusion if we have enough data\n            if vision_data and audio_data:\n                # Example fusion: combine visual and audio location\n                fused_result = self._fuse_vision_audio(vision_data, audio_data)\n                \n                # Process the fused result\n                self._act_on_fusion(fused_result)\n            \n            time.sleep(0.01)  # 100Hz fusion rate\n    \n    def _fuse_vision_audio(self, vision_data, audio_data):\n        """Example fusion of vision and audio data"""\n        # This is a simplified example\n        # In practice, this would use more sophisticated fusion algorithms\n        fusion_result = {\n            \'visual_objects\': len(vision_data[\'keypoints\']),\n            \'audio_features\': audio_data[\'features\'],\n            \'timestamp\': max(vision_data[\'timestamp\'], audio_data[\'timestamp\'])\n        }\n        return fusion_result\n    \n    def _act_on_fusion(self, fusion_result):\n        """Take action based on fused sensory data"""\n        # Example: print information about the environment\n        print(f"Detected {fusion_result[\'visual_objects\']} objects")\n        print(f"Audio features computed at {fusion_result[\'timestamp\']}")\n'})}),"\n",(0,t.jsx)(n.h2,{id:"integration-with-robot-control",children:"Integration with Robot Control"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def integrate_with_control_system(sensor_processor):\n    """Integrate sensory processing with robot control"""\n    \n    # Example: Use sensor data to control robot behavior\n    while sensor_processor.running:\n        # Get fused sensory data\n        # ... wait for new fusion result ...\n        \n        # Example: Navigate towards a sound source\n        # sound_direction = sensor_processor.get_sound_direction()\n        # robot.turn_towards(sound_direction)\n        \n        # Example: Avoid obstacles detected by vision\n        # obstacles = sensor_processor.get_vision_obstacles()\n        # if obstacles:\n        #     robot.avoid_obstacles(obstacles)\n        \n        time.sleep(0.01)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,t.jsx)(n.h3,{id:"profiling-and-optimization-techniques",children:"Profiling and Optimization Techniques"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import cProfile\nimport pstats\n\ndef profile_sensory_system():\n    """Profile the sensory processing system"""\n    profiler = cProfile.Profile()\n    \n    # Start processing\n    sensor_processor = RealtimeSensorProcessor()\n    sensor_processor.start_processing()\n    \n    # Profile for a few seconds\n    profiler.enable()\n    time.sleep(5)\n    profiler.disable()\n    \n    # Print results\n    stats = pstats.Stats(profiler)\n    stats.sort_stats(\'cumulative\')\n    stats.print_stats(10)  # Print top 10 functions\n    \n    sensor_processor.stop_processing()\n\n# Memory optimization example\ndef reduce_sensor_data_memory(data):\n    """Reduce memory usage of sensor data"""\n    # Use single precision instead of double\n    if isinstance(data, np.ndarray):\n        return data.astype(np.float32)\n    return data\n'})}),"\n",(0,t.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,t.jsx)(n.p,{children:"This chapter demonstrated practical implementations of sensory systems for humanoid robots. We covered vision processing, audio analysis, tactile sensing, and sensor fusion techniques. The examples provided can be adapted and extended based on specific robot hardware and application requirements."}),"\n",(0,t.jsx)(n.p,{children:"The implementations in this chapter emphasize real-time processing and computational efficiency, which are critical for practical deployment of sensory systems in humanoid robots. By following these examples, developers can build robust sensory systems that enable robots to effectively perceive and interact with their environment."})]})}function p(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>r,x:()=>o});var t=s(6540);const i={},a=t.createContext(i);function r(e){const n=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);