"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook=globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook||[]).push([[975],{7832:(e,i,n)=>{n.r(i),n.d(i,{assets:()=>r,contentTitle:()=>l,default:()=>h,frontMatter:()=>a,metadata:()=>s,toc:()=>c});var t=n(4848),o=n(8453);const a={},l=void 0,s={id:"textbook/chapter4/theory",title:"theory",description:"A* Algorithm",source:"@site/docs/textbook/chapter4/theory.md",sourceDirName:"textbook/chapter4",slug:"/textbook/chapter4/theory",permalink:"/Ai-book/docs/textbook/chapter4/theory",draft:!1,unlisted:!1,editUrl:"https://github.com/AftabAhmed4/Ai-book/edit/main/docs/textbook/chapter4/theory.md",tags:[],version:"current",frontMatter:{},sidebar:"textbookSidebar",previous:{title:"Chapter 4: Motion Planning and Navigation",permalink:"/Ai-book/docs/textbook/chapter4/intro"},next:{title:"Chapter 4: Practical Applications of Motion Planning and Navigation",permalink:"/Ai-book/docs/textbook/chapter4/practical"}},r={},c=[{value:"A* Algorithm",id:"a-algorithm",level:3},{value:"4.5 Optimization-Based Motion Planning",id:"45-optimization-based-motion-planning",level:2},{value:"Optimal Control Formulation",id:"optimal-control-formulation",level:3},{value:"Trajectory Optimization",id:"trajectory-optimization",level:3},{value:"4.6 Navigation Functions",id:"46-navigation-functions",level:2},{value:"Artificial Potential Fields",id:"artificial-potential-fields",level:3},{value:"Navigation Functions on Manifolds",id:"navigation-functions-on-manifolds",level:3},{value:"4.7 Control Theory for Navigation",id:"47-control-theory-for-navigation",level:2},{value:"Feedback Linearization",id:"feedback-linearization",level:3},{value:"Lyapunov-Based Control",id:"lyapunov-based-control",level:3},{value:"4.8 Uncertainty and Stochastic Planning",id:"48-uncertainty-and-stochastic-planning",level:2},{value:"Stochastic Motion Planning",id:"stochastic-motion-planning",level:3},{value:"Partially Observable Markov Decision Processes (POMDPs)",id:"partially-observable-markov-decision-processes-pomdps",level:3},{value:"4.9 Dynamic Environment Planning",id:"49-dynamic-environment-planning",level:2},{value:"Velocity Obstacles",id:"velocity-obstacles",level:3},{value:"Nonlinear Model Predictive Control (NMPC)",id:"nonlinear-model-predictive-control-nmpc",level:3},{value:"4.10 Multi-Robot Coordination",id:"410-multi-robot-coordination",level:2},{value:"Priority-Based Planning",id:"priority-based-planning",level:3},{value:"Conflict-Based Search (CBS)",id:"conflict-based-search-cbs",level:3},{value:"4.11 Computational Complexity",id:"411-computational-complexity",level:2},{value:"Complexity Analysis",id:"complexity-analysis",level:3},{value:"4.12 Convergence and Optimality",id:"412-convergence-and-optimality",level:2},{value:"Asymptotic Optimality",id:"asymptotic-optimality",level:3},{value:"Rate of Convergence",id:"rate-of-convergence",level:3},{value:"Conclusion",id:"conclusion",level:2}];function d(e){const i={code:"code",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(i.h3,{id:"a-algorithm",children:"A* Algorithm"}),"\n",(0,t.jsxs)(i.p,{children:["A* is a heuristic extension of Dijkstra's algorithm. It uses the evaluation function ",(0,t.jsx)(i.code,{children:"f(n) = g(n) + h(n)"}),", where:"]}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.code,{children:"g(n)"})," is the known cost from the start node to node ",(0,t.jsx)(i.code,{children:"n"})]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.code,{children:"h(n)"})," is the heuristic estimated cost from node ",(0,t.jsx)(i.code,{children:"n"})," to the goal"]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.code,{children:"f(n)"})," is the estimated total cost of the path through node ",(0,t.jsx)(i.code,{children:"n"})]}),"\n"]}),"\n",(0,t.jsxs)(i.p,{children:["A* is optimal if the heuristic ",(0,t.jsx)(i.code,{children:"h(n)"})," is admissible, meaning it never overestimates the true cost to the goal."]}),"\n",(0,t.jsx)(i.h2,{id:"45-optimization-based-motion-planning",children:"4.5 Optimization-Based Motion Planning"}),"\n",(0,t.jsx)(i.h3,{id:"optimal-control-formulation",children:"Optimal Control Formulation"}),"\n",(0,t.jsxs)(i.p,{children:["The motion planning problem can be formulated as an optimal control problem: minimize the integral of a cost function ",(0,t.jsx)(i.code,{children:"L(x(t), u(t), t)"})," from ",(0,t.jsx)(i.code,{children:"t = 0"})," to ",(0,t.jsx)(i.code,{children:"T"}),", subject to the system dynamics ",(0,t.jsx)(i.code,{children:"dx/dt = f(x(t), u(t), t)"}),", initial condition ",(0,t.jsx)(i.code,{children:"x(0) = x_start"}),", terminal condition ",(0,t.jsx)(i.code,{children:"x(T) = x_goal"}),", and inequality constraints ",(0,t.jsx)(i.code,{children:"g(x(t), u(t)) <= 0"}),"."]}),"\n",(0,t.jsx)(i.p,{children:"Here:"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.code,{children:"L"})," is the Lagrangian (running cost)"]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.code,{children:"f"})," defines the system dynamics"]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.code,{children:"g"})," represents path and control constraints"]}),"\n"]}),"\n",(0,t.jsx)(i.h3,{id:"trajectory-optimization",children:"Trajectory Optimization"}),"\n",(0,t.jsxs)(i.p,{children:["The continuous optimal control problem is discretized and solved as a nonlinear programming problem: minimize the sum of stage costs ",(0,t.jsx)(i.code,{children:"L(x_k, u_k)"})," subject to discrete dynamics ",(0,t.jsx)(i.code,{children:"x_{k+1} = f(x_k, u_k)"}),", initial state ",(0,t.jsx)(i.code,{children:"x_0 = x_start"}),", final state ",(0,t.jsx)(i.code,{children:"x_N = x_goal"}),", and constraints ",(0,t.jsx)(i.code,{children:"g(x_k, u_k) <= 0"}),"."]}),"\n",(0,t.jsx)(i.h2,{id:"46-navigation-functions",children:"4.6 Navigation Functions"}),"\n",(0,t.jsx)(i.h3,{id:"artificial-potential-fields",children:"Artificial Potential Fields"}),"\n",(0,t.jsx)(i.p,{children:"Artificial potential fields combine an attractive potential pulling the robot toward the goal and a repulsive potential pushing it away from obstacles."}),"\n",(0,t.jsxs)(i.p,{children:["The total potential is ",(0,t.jsx)(i.code,{children:"U_total = U_attr + U_rep"}),"."]}),"\n",(0,t.jsx)(i.p,{children:"The attractive potential is typically quadratic in the distance to the goal, while the repulsive potential is inversely related to the distance to the nearest obstacle and active only within a certain range."}),"\n",(0,t.jsxs)(i.p,{children:["The control input is the negative gradient of the total potential: ",(0,t.jsx)(i.code,{children:"u = -grad(U_attr + U_rep)"}),"."]}),"\n",(0,t.jsx)(i.h3,{id:"navigation-functions-on-manifolds",children:"Navigation Functions on Manifolds"}),"\n",(0,t.jsxs)(i.p,{children:["For smooth configuration space manifolds, navigation functions ",(0,t.jsx)(i.code,{children:"phi"})," are designed to be smooth and polar with:"]}),"\n",(0,t.jsxs)(i.ol,{children:["\n",(0,t.jsx)(i.li,{children:"a unique minimum at the goal configuration"}),"\n",(0,t.jsx)(i.li,{children:"no local minima other than the goal"}),"\n",(0,t.jsx)(i.li,{children:"values approaching infinity near obstacles"}),"\n"]}),"\n",(0,t.jsx)(i.p,{children:"Following the negative gradient of such a function guarantees convergence to the goal without getting trapped in local minima."}),"\n",(0,t.jsx)(i.h2,{id:"47-control-theory-for-navigation",children:"4.7 Control Theory for Navigation"}),"\n",(0,t.jsx)(i.h3,{id:"feedback-linearization",children:"Feedback Linearization"}),"\n",(0,t.jsx)(i.p,{children:"Feedback linearization transforms nonlinear robot dynamics into an equivalent linear system by canceling nonlinear terms with a suitable control input."}),"\n",(0,t.jsxs)(i.p,{children:["The resulting torque command is of the form:\n",(0,t.jsx)(i.code,{children:"tau = M(q) * x_ddot_des + C(q, q_dot) * q_dot + g(q) + feedback_terms"})]}),"\n",(0,t.jsxs)(i.p,{children:["Here ",(0,t.jsx)(i.code,{children:"K_p"})," and ",(0,t.jsx)(i.code,{children:"K_v"})," are positive definite gain matrices that shape the closed-loop error dynamics."]}),"\n",(0,t.jsx)(i.h3,{id:"lyapunov-based-control",children:"Lyapunov-Based Control"}),"\n",(0,t.jsxs)(i.p,{children:["Lyapunov-based methods design control laws that make a positive definite Lyapunov function ",(0,t.jsx)(i.code,{children:"V"})," decrease along system trajectories."]}),"\n",(0,t.jsxs)(i.p,{children:["A common choice is ",(0,t.jsx)(i.code,{children:"V(e) = 0.5 * e^T * e"}),", where ",(0,t.jsx)(i.code,{children:"e = q - q_des"})," is the configuration error. The control is designed such that ",(0,t.jsx)(i.code,{children:"dV/dt = e^T * de/dt < 0"}),", ensuring asymptotic stability."]}),"\n",(0,t.jsx)(i.h2,{id:"48-uncertainty-and-stochastic-planning",children:"4.8 Uncertainty and Stochastic Planning"}),"\n",(0,t.jsx)(i.h3,{id:"stochastic-motion-planning",children:"Stochastic Motion Planning"}),"\n",(0,t.jsxs)(i.p,{children:["In the presence of uncertainty, planning occurs in belief space. The objective is to minimize the expected cost subject to stochastic differential equations of the form:\n",(0,t.jsx)(i.code,{children:"dx = f(x, u, w) dt + g(x, u) dW"}),"\nwhere ",(0,t.jsx)(i.code,{children:"w"})," is process noise and ",(0,t.jsx)(i.code,{children:"W"})," is a Wiener process."]}),"\n",(0,t.jsx)(i.h3,{id:"partially-observable-markov-decision-processes-pomdps",children:"Partially Observable Markov Decision Processes (POMDPs)"}),"\n",(0,t.jsxs)(i.p,{children:["For partially observable environments, the problem is modeled as a POMDP. The optimal policy ",(0,t.jsx)(i.code,{children:"pi*"})," maximizes the expected discounted reward starting from initial belief ",(0,t.jsx)(i.code,{children:"b_0"}),", accounting for partial observability through belief state updates."]}),"\n",(0,t.jsx)(i.h2,{id:"49-dynamic-environment-planning",children:"4.9 Dynamic Environment Planning"}),"\n",(0,t.jsx)(i.h3,{id:"velocity-obstacles",children:"Velocity Obstacles"}),"\n",(0,t.jsxs)(i.p,{children:["The velocity obstacle ",(0,t.jsx)(i.code,{children:"VO_{i,j}"})," for a moving obstacle is the set of robot velocities ",(0,t.jsx)(i.code,{children:"v"})," that will lead to collision, assuming constant obstacle velocity ",(0,t.jsx)(i.code,{children:"v_j"}),". Selecting a control velocity outside all velocity obstacles ensures collision avoidance."]}),"\n",(0,t.jsx)(i.h3,{id:"nonlinear-model-predictive-control-nmpc",children:"Nonlinear Model Predictive Control (NMPC)"}),"\n",(0,t.jsx)(i.p,{children:"NMPC solves a finite-horizon optimal control problem at each time step, minimizing a cost over predicted states and controls subject to dynamics and constraints. The first control input is applied, and the process repeats at the next step with updated state measurements."}),"\n",(0,t.jsx)(i.h2,{id:"410-multi-robot-coordination",children:"4.10 Multi-Robot Coordination"}),"\n",(0,t.jsx)(i.h3,{id:"priority-based-planning",children:"Priority-Based Planning"}),"\n",(0,t.jsx)(i.p,{children:"Robots are assigned priorities and plan sequentially. Higher-priority robots plan ignoring lower-priority ones, while lower-priority robots treat higher-priority trajectories as dynamic obstacles."}),"\n",(0,t.jsx)(i.h3,{id:"conflict-based-search-cbs",children:"Conflict-Based Search (CBS)"}),"\n",(0,t.jsx)(i.p,{children:"CBS is a two-level algorithm:"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"High level: searches over conflict avoidance constraints between robot trajectories"}),"\n",(0,t.jsx)(i.li,{children:"Low level: plans individual collision-free paths using single-agent planners like A*"}),"\n"]}),"\n",(0,t.jsx)(i.h2,{id:"411-computational-complexity",children:"4.11 Computational Complexity"}),"\n",(0,t.jsx)(i.h3,{id:"complexity-analysis",children:"Complexity Analysis"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsxs)(i.li,{children:["Grid-based methods: ",(0,t.jsx)(i.code,{children:"O(n^2)"})," in 2D, ",(0,t.jsx)(i.code,{children:"O(n^3)"})," in 3D"]}),"\n",(0,t.jsx)(i.li,{children:"Sampling-based methods: offer probabilistic completeness with good practical performance"}),"\n",(0,t.jsx)(i.li,{children:"Exact cell decomposition: exponential in configuration space dimension in general"}),"\n",(0,t.jsxs)(i.li,{children:["Visibility graphs: ",(0,t.jsx)(i.code,{children:"O(n^2 log n)"})," for shortest paths in 2D polygonal environments"]}),"\n"]}),"\n",(0,t.jsx)(i.h2,{id:"412-convergence-and-optimality",children:"4.12 Convergence and Optimality"}),"\n",(0,t.jsx)(i.h3,{id:"asymptotic-optimality",children:"Asymptotic Optimality"}),"\n",(0,t.jsx)(i.p,{children:"Algorithms like RRT* are asymptotically optimal: as the number of samples goes to infinity, the cost of the returned solution converges in probability to the optimal cost."}),"\n",(0,t.jsx)(i.h3,{id:"rate-of-convergence",children:"Rate of Convergence"}),"\n",(0,t.jsx)(i.p,{children:"This refers to how quickly probabilistically complete planners find a feasible solution (if one exists) as the number of samples increases."}),"\n",(0,t.jsx)(i.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,t.jsx)(i.p,{children:"The theoretical foundations of motion planning provide the mathematical tools needed to develop robust navigation systems for humanoid robots. Understanding these concepts is essential for designing algorithms that can generate safe, efficient, and dynamically feasible paths in complex environments. The frameworks presented here form the basis for advanced planning approaches used in current humanoid robotics systems."})]})}function h(e={}){const{wrapper:i}={...(0,o.R)(),...e.components};return i?(0,t.jsx)(i,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,i,n)=>{n.d(i,{R:()=>l,x:()=>s});var t=n(6540);const o={},a=t.createContext(o);function l(e){const i=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function s(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:l(e.components),t.createElement(a.Provider,{value:i},e.children)}}}]);