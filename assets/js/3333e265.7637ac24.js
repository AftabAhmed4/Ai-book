"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook=globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook||[]).push([[41],{6089:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>p,frontMatter:()=>i,metadata:()=>s,toc:()=>c});var o=t(4848),a=t(8453);const i={},r="Chapter 5: Practical Applications of Human-Robot Interaction and Social Robotics",s={id:"textbook/chapter5/practical",title:"Chapter 5: Practical Applications of Human-Robot Interaction and Social Robotics",description:"Implementing Natural Language Interaction",source:"@site/docs/textbook/chapter5/practical.md",sourceDirName:"textbook/chapter5",slug:"/textbook/chapter5/practical",permalink:"/Ai-book/docs/textbook/chapter5/practical",draft:!1,unlisted:!1,editUrl:"https://github.com/AftabAhmed4/Ai-book/edit/main/docs/textbook/chapter5/practical.md",tags:[],version:"current",frontMatter:{},sidebar:"textbookSidebar",previous:{title:"Chapter 5: Theoretical Foundations of Human-Robot Interaction and Social Robotics",permalink:"/Ai-book/docs/textbook/chapter5/theory"},next:{title:"Chapter 5: Exercises and Problems",permalink:"/Ai-book/docs/textbook/chapter5/exercises"}},l={},c=[{value:"Implementing Natural Language Interaction",id:"implementing-natural-language-interaction",level:2},{value:"Speech Recognition and Natural Language Understanding",id:"speech-recognition-and-natural-language-understanding",level:3},{value:"Implementing Non-Verbal Communication",id:"implementing-non-verbal-communication",level:2},{value:"Gesture Recognition and Generation",id:"gesture-recognition-and-generation",level:3},{value:"Implementing Social Navigation",id:"implementing-social-navigation",level:2},{value:"Proxemic Behavior Implementation",id:"proxemic-behavior-implementation",level:3},{value:"Implementing Joint Attention Systems",id:"implementing-joint-attention-systems",level:2},{value:"Attention and Gaze Following",id:"attention-and-gaze-following",level:3},{value:"Implementing Emotional Interaction",id:"implementing-emotional-interaction",level:2},{value:"Emotion Recognition and Response",id:"emotion-recognition-and-response",level:3},{value:"Integrating Social Behaviors",id:"integrating-social-behaviors",level:2},{value:"Complete Social Interaction System",id:"complete-social-interaction-system",level:3},{value:"Evaluating Social Interaction Performance",id:"evaluating-social-interaction-performance",level:2},{value:"Metrics and Assessment",id:"metrics-and-assessment",level:3},{value:"Conclusion",id:"conclusion",level:2}];function m(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",p:"p",pre:"pre",...(0,a.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h1,{id:"chapter-5-practical-applications-of-human-robot-interaction-and-social-robotics",children:"Chapter 5: Practical Applications of Human-Robot Interaction and Social Robotics"}),"\n",(0,o.jsx)(n.h2,{id:"implementing-natural-language-interaction",children:"Implementing Natural Language Interaction"}),"\n",(0,o.jsx)(n.p,{children:"In this section, we'll implement practical systems for human-robot communication, starting with natural language processing."}),"\n",(0,o.jsx)(n.h3,{id:"speech-recognition-and-natural-language-understanding",children:"Speech Recognition and Natural Language Understanding"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import numpy as np\nimport speech_recognition as sr\nfrom typing import Dict, List, Tuple\nimport re\n\nclass NaturalLanguageInterface:\n    def __init__(self):\n        self.recognizer = sr.Recognizer()\n        self.microphone = sr.Microphone()\n        \n        # Define command patterns and their corresponding actions\n        self.command_patterns = {\n            'move_forward': [r'go forward', r'move forward', r'go straight', r'move straight'],\n            'move_backward': [r'go back', r'move back', r'go backward', r'move backward'],\n            'turn_left': [r'turn left', r'rotate left', r'move left'],\n            'turn_right': [r'turn right', r'rotate right', r'move right'],\n            'stop': [r'stop', r'halt', r'freeze', r'wait'],\n            'greet': [r'hello', r'hi', r'hey', r'greetings'],\n            'introduce': [r'tell me about yourself', r'who are you', r'what are you', r'introduce yourself']\n        }\n        \n        # Build regex patterns for each command\n        self.compiled_patterns = {}\n        for command, patterns in self.command_patterns.items():\n            combined_pattern = '|'.join(patterns)\n            self.compiled_patterns[command] = re.compile(combined_pattern, re.IGNORECASE)\n    \n    def listen_and_recognize(self) -> str:\n        \"\"\"Listen to user speech and return recognized text\"\"\"\n        try:\n            with self.microphone as source:\n                self.recognizer.adjust_for_ambient_noise(source)\n                print(\"Listening...\")\n                audio = self.recognizer.listen(source, timeout=5)\n            \n            # Use Google's speech recognition service\n            text = self.recognizer.recognize_google(audio)\n            print(f\"Heard: {text}\")\n            return text\n        except sr.WaitTimeoutError:\n            print(\"No speech detected\")\n            return \"\"\n        except sr.UnknownValueError:\n            print(\"Could not understand audio\")\n            return \"\"\n        except sr.RequestError as e:\n            print(f\"Could not request results; {e}\")\n            return \"\"\n    \n    def parse_command(self, text: str) -> str:\n        \"\"\"Parse text and determine the corresponding command\"\"\"\n        for command, pattern in self.compiled_patterns.items():\n            if pattern.search(text):\n                return command\n        return 'unknown'  # Command not recognized\n    \n    def generate_response(self, command: str) -> str:\n        \"\"\"Generate appropriate verbal response for a command\"\"\"\n        responses = {\n            'greet': \"Hello! Nice to meet you!\",\n            'introduce': \"I am a humanoid robot designed to interact with humans. I can move, talk, and assist with various tasks.\",\n            'move_forward': \"Moving forward now.\",\n            'move_backward': \"Moving backward now.\",\n            'turn_left': \"Turning left now.\",\n            'turn_right': \"Turning right now.\",\n            'stop': \"Stopping now.\",\n            'unknown': \"I'm sorry, I didn't understand that command.\"\n        }\n        return responses.get(command, \"I don't know how to respond to that.\")\n\n# Example usage\nnli = NaturalLanguageInterface()\n\n# In a complete system, this would be called in a loop\n# user_text = nli.listen_and_recognize()\n# command = nli.parse_command(user_text)\n# response = nli.generate_response(command)\n# print(f\"Command: {command}, Response: {response}\")\n"})}),"\n",(0,o.jsx)(n.h2,{id:"implementing-non-verbal-communication",children:"Implementing Non-Verbal Communication"}),"\n",(0,o.jsx)(n.h3,{id:"gesture-recognition-and-generation",children:"Gesture Recognition and Generation"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import numpy as np\nimport cv2\nfrom typing import List, Tuple\nimport mediapipe as mp\n\nclass GestureRecognition:\n    def __init__(self):\n        # Initialize MediaPipe for hand tracking\n        self.mp_hands = mp.solutions.hands\n        self.mp_drawing = mp.solutions.drawing_utils\n        self.hands = self.mp_hands.Hands(\n            static_image_mode=False,\n            max_num_hands=2,\n            min_detection_confidence=0.5,\n            min_tracking_confidence=0.5\n        )\n        \n        # Gesture recognition parameters\n        self.gesture_thresholds = {\n            \'thumb_up\': 0.9,   # Confidence for thumbs up gesture\n            \'open_palm\': 0.85, # Confidence for open palm gesture\n            \'pointing\': 0.8    # Confidence for pointing gesture\n        }\n    \n    def recognize_hand_gesture(self, image: np.ndarray) -> str:\n        """Recognize hand gestures from an image"""\n        # Convert image to RGB (MediaPipe expects RGB)\n        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        # Process the image\n        results = self.hands.process(image_rgb)\n        \n        if results.multi_hand_landmarks:\n            for hand_landmarks in results.multi_hand_landmarks:\n                # Get coordinates of key landmarks\n                landmarks = []\n                for landmark in hand_landmarks.landmark:\n                    landmarks.append([landmark.x, landmark.y, landmark.z])\n                \n                # Analyze gesture based on landmark positions\n                gesture = self._analyze_gesture(landmarks)\n                if gesture:\n                    # Draw landmarks for visualization\n                    self.mp_drawing.draw_landmarks(\n                        image, hand_landmarks, self.mp_hands.HAND_CONNECTIONS)\n                    return gesture\n        \n        return "none"\n    \n    def _analyze_gesture(self, landmarks: List[List[float]]) -> str:\n        """Analyze landmark positions to identify gesture"""\n        if len(landmarks) < 21:  # MediaPipe provides 21 landmarks per hand\n            return "none"\n        \n        # Extract key landmark positions\n        wrist = landmarks[0]\n        thumb_tip = landmarks[4]\n        index_tip = landmarks[8]\n        middle_tip = landmarks[12]\n        ring_tip = landmarks[16]\n        pinky_tip = landmarks[20]\n        \n        # Thumb up: thumb tip is above other fingertips\n        if (thumb_tip[1] < index_tip[1] and \n            thumb_tip[1] < middle_tip[1] and \n            thumb_tip[1] < ring_tip[1]):\n            return "thumb_up"\n        \n        # Open palm: all fingertips are higher (lower y-value) than wrists\n        if (index_tip[1] < wrist[1] and \n            middle_tip[1] < wrist[1] and \n            ring_tip[1] < wrist[1] and \n            pinky_tip[1] < wrist[1]):\n            return "open_palm"\n        \n        # Pointing: only index finger extended\n        if (index_tip[1] < wrist[1] and \n            middle_tip[1] > wrist[1] and \n            ring_tip[1] > wrist[1] and \n            pinky_tip[1] > wrist[1]):\n            return "pointing"\n        \n        return "unknown"\n    \n    def generate_robot_gesture(self, gesture_type: str) -> str:\n        """Generate appropriate robot response to a human gesture"""\n        responses = {\n            "thumb_up": "Responding with positive confirmation gesture",\n            "open_palm": "Responding with open gesture to indicate friendliness",\n            "pointing": "Looking in the direction pointed by the human",\n            "unknown": "Cannot interpret this gesture"\n        }\n        return responses.get(gesture_type, "No response for this gesture")\n\n# Example usage\ngesture_rec = GestureRecognition()\n# Assuming we have an image from robot\'s camera\n# gesture = gesture_rec.recognize_hand_gesture(robot_camera_image)\n# robot_response = gesture_rec.generate_robot_gesture(gesture)\n'})}),"\n",(0,o.jsx)(n.h2,{id:"implementing-social-navigation",children:"Implementing Social Navigation"}),"\n",(0,o.jsx)(n.h3,{id:"proxemic-behavior-implementation",children:"Proxemic Behavior Implementation"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import numpy as np\nfrom typing import List, Tuple, Dict\nfrom enum import Enum\n\nclass ProxemicZone(Enum):\n    INTIMATE = (0.0, 0.45)    # 0-1.5 ft\n    PERSONAL = (0.45, 1.2)    # 1.5-4 ft\n    SOCIAL = (1.2, 3.7)       # 4-12 ft\n    PUBLIC = (3.7, float(\'inf\')) # 12+ ft\n\nclass SocialNavigationController:\n    def __init__(self, robot_radius: float = 0.3):\n        self.robot_radius = robot_radius  # Robot\'s physical radius\n        self.human_positions: List[Tuple[float, float]] = []\n        self.social_zones: Dict[Tuple[float, float], ProxemicZone] = {}\n        self.comfort_distances = {\n            ProxemicZone.INTIMATE: 0.25,\n            ProxemicZone.PERSONAL: 0.6,\n            ProxemicZone.SOCIAL: 1.5,\n            ProxemicZone.PUBLIC: 4.0\n        }\n        \n    def update_human_positions(self, positions: List[Tuple[float, float]]):\n        """Update known human positions"""\n        self.human_positions = positions\n        self._update_social_zones()\n    \n    def _update_social_zones(self):\n        """Update the social zone classification for each human"""\n        self.social_zones = {}\n        robot_pos = self.get_robot_position()  # Implemented in full system\n        \n        for human_pos in self.human_positions:\n            dist = np.sqrt((robot_pos[0] - human_pos[0])**2 + (robot_pos[1] - human_pos[1])**2)\n            \n            # Determine which zone this distance represents\n            zone = ProxemicZone.PUBLIC  # Default to public zone\n            for zone_enum in ProxemicZone:\n                min_d, max_d = zone_enum.value\n                if min_d <= dist < max_d:\n                    zone = zone_enum\n                    break\n            \n            self.social_zones[human_pos] = zone\n    \n    def get_robot_position(self) -> Tuple[float, float]:\n        """Get current robot position (mock implementation)"""\n        # In a real system, this would interface with robot localization\n        return (0.0, 0.0)\n    \n    def calculate_desired_position(self, target_pos: Tuple[float, float]) -> Tuple[float, float]:\n        """Calculate a socially acceptable position that respects human space"""\n        robot_pos = self.get_robot_position()\n        \n        # Check if moving toward target would violate proxemics\n        for human_pos in self.human_positions:\n            # Calculate distance to human if we were at target position\n            dist_to_human = np.sqrt((target_pos[0] - human_pos[0])**2 + \n                                   (target_pos[1] - human_pos[1])**2)\n            \n            # Check if too close to human (less than personal space)\n            if dist_to_human < self.comfort_distances[ProxemicZone.PERSONAL]:\n                # Calculate vector from human to robot\n                vec_human_to_robot = np.array(robot_pos) - np.array(human_pos)\n                vec_human_to_robot = vec_human_to_robot / np.linalg.norm(vec_human_to_robot)\n                \n                # Position robot outside comfort distance\n                offset = vec_human_to_robot * self.comfort_distances[ProxemicZone.PERSONAL]\n                adjusted_pos = np.array(human_pos) + offset\n                return tuple(adjusted_pos)\n        \n        # If no conflicts, return original target\n        return target_pos\n    \n    def adjust_navigation_path(self, original_path: List[Tuple[float, float]]) -> List[Tuple[float, float]]:\n        """Adjust navigation path to respect social boundaries"""\n        adjusted_path = []\n        \n        for point in original_path:\n            adjusted_point = self.calculate_desired_position(point)\n            adjusted_path.append(adjusted_point)\n        \n        return adjusted_path\n    \n    def get_behavior_for_zone(self, zone: ProxemicZone) -> str:\n        """Get appropriate behavior based on proxemic zone"""\n        behaviors = {\n            ProxemicZone.INTIMATE: "Stop immediately - too close",\n            ProxemicZone.PERSONAL: "Slow down and ask permission to approach",\n            ProxemicZone.SOCIAL: "Maintain polite distance, appropriate for interaction",\n            ProxemicZone.PUBLIC: "Normal navigation speed"\n        }\n        return behaviors.get(zone, "Unknown zone behavior")\n\n# Example usage\nsocial_nav = SocialNavigationController(robot_radius=0.4)\nhumans = [(1.0, 0.5), (2.5, 1.2), (-0.5, -1.0)]\nsocial_nav.update_human_positions(humans)\n\ntarget = (3.0, 3.0)\nadjusted_target = social_nav.calculate_desired_position(target)\nprint(f"Adjusted target position: {adjusted_target}")\n'})}),"\n",(0,o.jsx)(n.h2,{id:"implementing-joint-attention-systems",children:"Implementing Joint Attention Systems"}),"\n",(0,o.jsx)(n.h3,{id:"attention-and-gaze-following",children:"Attention and Gaze Following"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import numpy as np\nfrom typing import Tuple, List, Optional\nimport cv2\n\nclass JointAttentionSystem:\n    def __init__(self):\n        self.robot_position: Tuple[float, float, float] = (0.0, 0.0, 1.5)  # x, y, height\n        self.robot_orientation: float = 0.0  # Heading in radians\n        self.attended_objects = []\n        self.human_attention_targets = []\n        self.gaze_following_enabled = True\n        \n    def detect_human_gaze_direction(self, face_landmarks: List[Tuple[int, int]], \n                                  head_pose: Tuple[float, float, float]) -> Optional[Tuple[float, float]]:\n        """Calculate where human is looking based on face landmarks and head pose"""\n        # Simplified model - in practice, this would use more sophisticated eye tracking\n        if len(face_landmarks) < 68:\n            return None\n        \n        # Extract eye landmarks (simplified)\n        # In a real system, use proper eye tracking with head pose compensation\n        left_eye_center = face_landmarks[36:42]  # Approximate eye region\n        right_eye_center = face_landmarks[42:48]  # Approximate eye region\n        \n        # Calculate center of eyes\n        left_eye_avg = np.mean(left_eye_center, axis=0)\n        right_eye_avg = np.mean(right_eye_center, axis=0)\n        eye_center = (left_eye_avg + right_eye_avg) / 2\n        \n        # Calculate gaze direction based on head rotation\n        yaw, pitch, roll = head_pose\n        \n        # Simplified gaze direction calculation\n        # In practice, use 3D eye model with head pose compensation\n        gaze_x = np.cos(yaw) * np.cos(pitch)\n        gaze_y = np.sin(yaw) * np.cos(pitch)\n        gaze_z = np.sin(pitch)\n        \n        # Normalize\n        gaze_vector = np.array([gaze_x, gaze_y, gaze_z])\n        gaze_vector = gaze_vector / np.linalg.norm(gaze_vector)\n        \n        return tuple(gaze_vector)\n    \n    def find_attention_target(self, gaze_direction: Tuple[float, float, float], \n                            field_of_view: float = 45.0) -> Optional[Tuple[float, float]]:\n        """Determine what the human is looking at"""\n        # Convert field of view to radians\n        fov_rad = np.radians(field_of_view)\n        \n        # In a complete system, this would perform ray-object intersection tests\n        # with known objects in the environment\n        \n        # For this example, just return a point in the gaze direction\n        robot_pos = np.array(self.robot_position)\n        gaze_vec = np.array(gaze_direction)\n        \n        # Calculate a point in the distance where the human is looking\n        look_at_point = robot_pos + 5.0 * gaze_vec  # Look 5m ahead\n        \n        return (look_at_point[0], look_at_point[1])\n    \n    def follow_human_gaze(self, human_gaze_target: Tuple[float, float]) -> bool:\n        """Turn robot to look at the same location as the human"""\n        if not self.gaze_following_enabled:\n            return False\n        \n        # Calculate robot\'s relative position to target\n        robot_pos = np.array([self.robot_position[0], self.robot_position[1]])\n        target_pos = np.array(human_gaze_target)\n        \n        # Calculate direction vector\n        direction_vector = target_pos - robot_pos\n        distance = np.linalg.norm(direction_vector)\n        \n        # Check if target is within a reasonable distance\n        if distance > 10.0:  # Too far to meaningfully look at\n            return False\n        \n        # Normalize the direction vector\n        direction_unit = direction_vector / distance\n        \n        # Calculate required rotation angle\n        current_heading = np.array([\n            np.cos(self.robot_orientation), \n            np.sin(self.robot_orientation)\n        ])\n        \n        # Calculate angle difference\n        cos_angle = np.clip(np.dot(current_heading, direction_unit), -1.0, 1.0)\n        angle_diff = np.arccos(cos_angle)\n        \n        # Determine rotation direction\n        cross_product = current_heading[0] * direction_unit[1] - current_heading[1] * direction_unit[0]\n        if cross_product < 0:\n            angle_diff = -angle_diff\n        \n        # In a real system, this would command the robot\'s head/eyes to turn\n        print(f"Turning to look at {human_gaze_target}, rotation: {angle_diff:.2f} radians")\n        \n        # Update robot orientation\n        self.robot_orientation += angle_diff\n        return True\n    \n    def direct_human_attention(self, object_pos: Tuple[float, float]) -> bool:\n        """Direct human attention to a specific object"""\n        # Calculate direction to object relative to human\n        # In a real system, this might involve pointing with arm or looking first\n        \n        robot_pos = np.array([self.robot_position[0], self.robot_position[1]])\n        obj_pos = np.array(object_pos)\n        \n        # Vector from robot to object\n        robot_to_obj = obj_pos - robot_pos\n        robot_to_obj_norm = robot_to_obj / np.linalg.norm(robot_to_obj)\n        \n        # In a complete system, this would involve arm gestures or other attention-directing actions\n        print(f"Directing attention to object at {object_pos}")\n        return True\n    \n    def maintain_joint_attention(self, target_object: Tuple[float, float]) -> bool:\n        """Maintain joint visual attention on an object with human"""\n        # Both robot and human should be looking at the same object\n        human_attention = self.find_human_attention()\n        robot_attention = target_object\n        \n        if human_attention and np.allclose(human_attention, robot_attention, atol=0.5):\n            print("Joint attention achieved")\n            return True\n        \n        print("Establishing joint attention")\n        if human_attention:\n            self.follow_human_gaze(human_attention)\n        else:\n            self.direct_human_attention(target_object)\n        \n        return False\n    \n    def find_human_attention(self) -> Optional[Tuple[float, float]]:\n        """Mock function to get human attention target"""\n        # In a real system, this would come from the gaze detection system\n        if self.human_attention_targets:\n            return self.human_attention_targets[-1]  # Return most recent\n        return None\n\n# Example usage\njag_system = JointAttentionSystem()\n\n# Simulate human gaze direction (in real system, this would come from face tracking)\nhuman_gaze_dir = (0.7, 0.3, 0.6)  # Normalized direction vector\nattention_target = jag_system.find_attention_target(human_gaze_dir)\n\nif attention_target:\n    jag_system.follow_human_gaze(attention_target)\n'})}),"\n",(0,o.jsx)(n.h2,{id:"implementing-emotional-interaction",children:"Implementing Emotional Interaction"}),"\n",(0,o.jsx)(n.h3,{id:"emotion-recognition-and-response",children:"Emotion Recognition and Response"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import numpy as np\nimport cv2\nfrom typing import Dict, Tuple, List\nfrom enum import Enum\n\nclass EmotionalState(Enum):\n    HAPPY = 1\n    SAD = 2\n    ANGRY = 3\n    FEARFUL = 4\n    SURPRISED = 5\n    NEUTRAL = 6\n\nclass EmotionalInteractionSystem:\n    def __init__(self):\n        # Using OpenCV\'s face detection and a mock emotion classifier\n        self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \'haarcascade_frontalface_default.xml\')\n        \n        # Emotion recognition weights (in practice, use a trained model)\n        self.emotion_weights = {\n            EmotionalState.HAPPY: np.array([0.1, 0.9, 0.2, 0.8]),  # Smile, eye shape features\n            EmotionalState.SAD: np.array([0.8, 0.2, 0.7, 0.1]),    # Downward mouth, eye features\n            EmotionalState.ANGRY: np.array([0.9, 0.1, 0.1, 0.9]),  # Furrowed brows, tight mouth\n            EmotionalState.NEUTRAL: np.array([0.5, 0.5, 0.5, 0.5]) # Balanced features\n        }\n        \n        # Current emotional state of humans in the environment\n        self.human_emotions: Dict[Tuple[int, int, int, int], EmotionalState] = {}  # face bounding box -> emotion\n        \n        # Robot\'s emotional responses\n        self.emotional_responses = {\n            EmotionalState.HAPPY: "I\'m glad you\'re happy! What\'s making you feel this way?",\n            EmotionalState.SAD: "I can see you\'re feeling sad. Would you like to talk about it?",\n            EmotionalState.ANGRY: "I notice you seem upset. How can I help?",\n            EmotionalState.FEARFUL: "You seem worried. I\'m here to help if I can.",\n            EmotionalState.SURPRISED: "Oh, that seems unexpected! What happened?",\n            EmotionalState.NEUTRAL: "Hello! How can I assist you today?"\n        }\n    \n    def detect_faces(self, image: np.ndarray) -> List[Tuple[int, int, int, int]]:\n        """Detect faces in an image using Haar cascade"""\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        faces = self.face_cascade.detectMultiScale(\n            gray,\n            scaleFactor=1.1,\n            minNeighbors=5,\n            minSize=(30, 30)\n        )\n        return [(x, y, w, h) for (x, y, w, h) in faces]\n    \n    def recognize_emotion(self, face_roi: np.ndarray) -> EmotionalState:\n        """Recognize emotion from a face region of interest"""\n        # Resize to standard size for processing\n        face_resized = cv2.resize(face_roi, (48, 48))\n        gray_face = cv2.cvtColor(face_resized, cv2.COLOR_BGR2GRAY)\n        \n        # Extract simple features (in practice, use a CNN)\n        # These features are simplified representations of facial features\n        features = self._extract_features(gray_face)\n        \n        # Compare features to emotion weights\n        emotion_scores = {}\n        for emotion, weights in self.emotion_weights.items():\n            score = np.dot(features[:len(weights)], weights)\n            emotion_scores[emotion] = score\n        \n        # Return the emotion with highest score\n        predicted_emotion = max(emotion_scores, key=emotion_scores.get)\n        return predicted_emotion\n    \n    def _extract_features(self, face_image: np.ndarray) -> np.ndarray:\n        """Extract simple facial features for emotion recognition"""\n        # This is a simplified feature extraction\n        # In practice, use deep learning models or more sophisticated features\n        \n        # Get some representative pixel values from key facial regions\n        h, w = face_image.shape\n        features = np.zeros(10)\n        \n        # Sample from different regions\n        features[0] = face_image[h//4, w//4]  # Forehead\n        features[1] = face_image[h//2, w//4]  # Eye region left\n        features[2] = face_image[h//2, 3*w//4]  # Eye region right\n        features[3] = face_image[3*h//4, w//4]  # Mouth left\n        features[4] = face_image[3*h//4, w//2]  # Mouth center\n        features[5] = face_image[3*h//4, 3*w//4]  # Mouth right\n        features[6] = face_image[h//3, w//2]    # Nose bridge\n        features[7] = face_image[2*h//3, w//3]  # Cheek left\n        features[8] = face_image[2*h//3, 2*w//3] # Cheek right\n        features[9] = np.mean(face_image)       # Overall average brightness\n        \n        # Normalize features\n        features = features / 255.0\n        return features\n    \n    def process_emotions_in_image(self, image: np.ndarray) -> Dict[Tuple[int, int, int, int], EmotionalState]:\n        """Process image to recognize emotions of all detected faces"""\n        faces = self.detect_faces(image)\n        emotions = {}\n        \n        for (x, y, w, h) in faces:\n            # Extract face region of interest\n            face_roi = image[y:y+h, x:x+w]\n            \n            # Recognize emotion for this face\n            emotion = self.recognize_emotion(face_roi)\n            emotions[(x, y, w, h)] = emotion\n            \n            # Draw bounding rectangle and label\n            cv2.rectangle(image, (x, y), (x+w, y+h), (255, 0, 0), 2)\n            cv2.putText(image, emotion.name, (x, y-10), \n                       cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2)\n        \n        # Store the new emotions\n        self.human_emotions = emotions\n        return emotions\n    \n    def generate_response_to_emotion(self, emotion: EmotionalState) -> str:\n        """Generate appropriate response to human emotion"""\n        return self.emotional_responses.get(emotion, "I\'m not sure how to respond to this emotion.")\n    \n    def adapt_behavior_to_emotions(self) -> str:\n        """Adapt robot behavior based on detected emotions"""\n        # Find the dominant emotion among detected humans\n        if not self.human_emotions:\n            return self.emotional_responses[EmotionalState.NEUTRAL]\n        \n        # Count emotion occurrences\n        emotion_counts = {}\n        for emotion in self.human_emotions.values():\n            emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1\n        \n        # Find most common emotion\n        dominant_emotion = max(emotion_counts, key=emotion_counts.get)\n        \n        # Generate appropriate response\n        response = self.generate_response_to_emotion(dominant_emotion)\n        \n        # Adjust behavior based on emotion\n        if dominant_emotion in [EmotionalState.ANGRY, EmotionalState.SAD]:\n            print("Adjusting to calm and supportive behavior mode")\n        elif dominant_emotion == EmotionalState.HAPPY:\n            print("Adjusting to more energetic and engaging behavior mode")\n        \n        return response\n\n# Example usage\nemotion_sys = EmotionalInteractionSystem()\n# This would process camera images in a real system\n# emotions = emotion_sys.process_emotions_in_image(robot_camera_image)\n# response = emotion_sys.adapt_behavior_to_emotions()\n# print(f"Response: {response}")\n'})}),"\n",(0,o.jsx)(n.h2,{id:"integrating-social-behaviors",children:"Integrating Social Behaviors"}),"\n",(0,o.jsx)(n.h3,{id:"complete-social-interaction-system",children:"Complete Social Interaction System"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import time\nimport threading\nfrom typing import List, Tuple, Dict, Optional\n\nclass SocialInteractionSystem:\n    def __init__(self):\n        # Initialize subsystems\n        self.nli = NaturalLanguageInterface()  # From earlier implementation\n        self.gesture_rec = GestureRecognition()  # From earlier implementation\n        self.social_nav = SocialNavigationController()  # From earlier implementation\n        self.jag_system = JointAttentionSystem()  # From earlier implementation\n        self.emotion_sys = EmotionalInteractionSystem()  # From earlier implementation\n        \n        # Social interaction state\n        self.interaction_active = False\n        self.interaction_partner = None\n        self.interaction_history = []\n        self.social_preferences = {}\n        \n        # Main control loop parameters\n        self.control_frequency = 10  # Hz\n        self.control_thread = None\n        self.running = False\n    \n    def start_interaction_loop(self):\n        """Start the main social interaction control loop"""\n        self.running = True\n        self.control_thread = threading.Thread(target=self._interaction_loop)\n        self.control_thread.start()\n    \n    def stop_interaction_loop(self):\n        """Stop the main social interaction control loop"""\n        self.running = False\n        if self.control_thread:\n            self.control_thread.join()\n    \n    def _interaction_loop(self):\n        """Main loop for handling social interactions"""\n        dt = 1.0 / self.control_frequency\n        \n        while self.running:\n            start_time = time.time()\n            \n            # Process environment inputs\n            self._process_sensory_inputs()\n            \n            # Update social state\n            self._update_social_context()\n            \n            # Generate appropriate responses\n            self._generate_social_responses()\n            \n            # Respect control loop timing\n            elapsed = time.time() - start_time\n            sleep_time = max(0, dt - elapsed)\n            if sleep_time > 0:\n                time.sleep(sleep_time)\n    \n    def _process_sensory_inputs(self):\n        """Process all sensory inputs for social interaction"""\n        # Note: In a real implementation, these would come from actual sensors\n        # For this example, we\'ll use simulated data\n        \n        # Simulate camera input for face/gesture detection\n        # camera_image = get_robot_camera_image()  # Would be from actual camera\n        \n        # Process emotional expressions\n        # emotions = self.emotion_sys.process_emotions_in_image(camera_image)\n        \n        # Process gestures\n        # gesture = self.gesture_rec.recognize_hand_gesture(camera_image)\n        \n        # Process audio input\n        # text_input = self.nli.listen_and_recognize()\n        # if text_input:\n        #     command = self.nli.parse_command(text_input)\n        #     self.interaction_history.append((\'human_speech\', text_input, command))\n        \n        # Process human positions for spatial awareness\n        # human_positions = get_detected_human_positions()  # From perception system\n        # self.social_nav.update_human_positions(human_positions)\n    \n    def _update_social_context(self):\n        """Update the social context based on all inputs"""\n        # Determine if there\'s a primary interaction partner\n        if hasattr(self, \'detected_humans\') and self.detected_humans:\n            # Select the closest human as primary interaction partner\n            robot_pos = self.social_nav.get_robot_position()\n            closest_human = min(self.detected_humans, \n                               key=lambda h: np.sqrt((h[0]-robot_pos[0])**2 + (h[1]-robot_pos[1])**2))\n            self.interaction_partner = closest_human\n    \n    def _generate_social_responses(self):\n        """Generate appropriate social responses based on context"""\n        if not self.interaction_partner:\n            return  # No one to interact with\n        \n        # Check for new inputs requiring responses\n        for interaction_type, data, metadata in reversed(self.interaction_history[-5:]):  # Check last 5 interactions\n            if interaction_type == \'speech\' and metadata == \'greet\':\n                response = self.nli.generate_response(\'greet\')\n                self._execute_verbal_response(response)\n                \n            elif interaction_type == \'gesture\' and data == \'open_palm\':\n                gesture_response = self.gesture_rec.generate_robot_gesture(\'open_palm\')\n                self._execute_gesture_response(gesture_response)\n                \n            elif interaction_type == \'emotion\' and data in [EmotionalState.SAD, EmotionalState.ANGRY]:\n                emotional_response = self.emotion_sys.generate_response_to_emotion(data)\n                self._execute_emotional_response(emotional_response)\n    \n    def _execute_verbal_response(self, response_text: str):\n        """Execute verbal response (text-to-speech in real system)"""\n        print(f"Robot says: {response_text}")\n        # In a real system, this would use text-to-speech\n    \n    def _execute_gesture_response(self, response_description: str):\n        """Execute gesture response (robot motion in real system)"""\n        print(f"Robot gesture: {response_description}")\n        # In a real system, this would command robot actuators\n    \n    def _execute_emotional_response(self, response_text: str):\n        """Execute appropriate response to emotional state"""\n        print(f"Emotional response: {response_text}")\n        # In a real system, this might adjust robot behavior, facial expressions, etc.\n    \n    def greet_human(self, human_position: Tuple[float, float]):\n        """Greet a human at a specific position"""\n        # Approach within social distance\n        comfort_distance = self.social_nav.comfort_distances[ProxemicZone.SOCIAL]\n        approach_vector = np.array(human_position) - np.array(self.social_nav.get_robot_position())\n        approach_distance = np.linalg.norm(approach_vector)\n        \n        if approach_distance > comfort_distance:\n            # Move closer to appropriate social distance\n            approach_direction = approach_vector / approach_distance\n            target_position = np.array(human_position) - approach_direction * comfort_distance\n            print(f"Approaching to {target_position} to greet human")\n        \n        # Execute greeting behavior\n        greeting_speech = "Hello! It\'s nice to meet you."\n        self._execute_verbal_response(greeting_speech)\n        \n        # Add to interaction history\n        self.interaction_history.append((\'greeting\', human_position, time.time()))\n    \n    def request_attention(self, human_position: Tuple[float, float]):\n        """Request attention from a human"""\n        # Use appropriate attention-getting behavior\n        print(f"Requesting attention from human at {human_position}")\n        \n        # In a real system, this might involve:\n        # - Gentle movement to enter their visual field\n        # - Audible attention-getting sound\n        # - LED indicators\n        # - Wait for attention acknowledgment before proceeding\n\n# Example usage\nsocial_system = SocialInteractionSystem()\nsocial_system.start_interaction_loop()\n\n# Simulate greeting a human\nhuman_pos = (2.0, 1.5)\nsocial_system.greet_human(human_pos)\n\n# Stop the system after demonstration\ntime.sleep(1)\nsocial_system.stop_interaction_loop()\n'})}),"\n",(0,o.jsx)(n.h2,{id:"evaluating-social-interaction-performance",children:"Evaluating Social Interaction Performance"}),"\n",(0,o.jsx)(n.h3,{id:"metrics-and-assessment",children:"Metrics and Assessment"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'from dataclasses import dataclass\nfrom typing import List\nimport time\n\n@dataclass\nclass InteractionMetrics:\n    engagement_duration: float\n    response_accuracy: float\n    social_acceptability: float\n    task_success_rate: float\n    user_satisfaction: float\n\nclass SocialInteractionEvaluator:\n    def __init__(self):\n        self.metrics_history: List[InteractionMetrics] = []\n        self.start_time = None\n        self.interactions_count = 0\n    \n    def start_session(self):\n        """Start a new evaluation session"""\n        self.start_time = time.time()\n        self.interactions_count = 0\n    \n    def evaluate_engagement(self, robot_behavior: str, human_response: str) -> float:\n        """Evaluate the quality of engagement"""\n        # Simplified evaluation - in practice, this would use more sophisticated metrics\n        positive_responses = [\'yes\', \'okay\', \'please\', \'thank you\', \'hello\', \'smile\', \'nod\']\n        negative_responses = [\'no\', \'stop\', \'away\', \'frown\', \'shake_head\']\n        \n        engagement_score = 0.5  # Base score\n        \n        if any(pos in human_response.lower() for pos in positive_responses):\n            engagement_score += 0.3\n        elif any(neg in human_response.lower() for neg in negative_responses):\n            engagement_score -= 0.3\n        \n        return max(0.0, min(1.0, engagement_score))\n    \n    def evaluate_social_acceptability(self, human_distance: float, robot_behavior: str) -> float:\n        """Evaluate if robot behavior is socially acceptable"""\n        # Based on proxemic zones\n        if 0.45 <= human_distance <= 1.2 and robot_behavior == "normal_operation":\n            return 0.8  # Acceptable social distance\n        elif human_distance < 0.45:  # Too close\n            return 0.2 if robot_behavior != "apologizing" else 0.6\n        else:\n            return 0.6  # Could be more engaging\n    \n    def record_interaction(self, metrics: InteractionMetrics):\n        """Record metrics for an interaction"""\n        self.metrics_history.append(metrics)\n        self.interactions_count += 1\n    \n    def get_overall_performance(self) -> InteractionMetrics:\n        """Calculate overall performance metrics"""\n        if not self.metrics_history:\n            return InteractionMetrics(0.0, 0.0, 0.0, 0.0, 0.0)\n        \n        avg_duration = sum(m.engagement_duration for m in self.metrics_history) / len(self.metrics_history)\n        avg_accuracy = sum(m.response_accuracy for m in self.metrics_history) / len(self.metrics_history)\n        avg_acceptability = sum(m.social_acceptability for m in self.metrics_history) / len(self.metrics_history)\n        avg_success = sum(m.task_success_rate for m in self.metrics_history) / len(self.metrics_history)\n        avg_satisfaction = sum(m.user_satisfaction for m in self.metrics_history) / len(self.metrics_history)\n        \n        return InteractionMetrics(\n            avg_duration, avg_accuracy, avg_acceptability, avg_success, avg_satisfaction\n        )\n'})}),"\n",(0,o.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,o.jsx)(n.p,{children:"This chapter provided practical implementations of human-robot interaction and social robotics concepts. We covered natural language processing, non-verbal communication, social navigation, joint attention systems, and emotional interaction. The implementations include complete systems that integrate multiple social behaviors."}),"\n",(0,o.jsx)(n.p,{children:"The examples in this chapter demonstrate how theoretical concepts from psychology, cognitive science, and social science translate into working code for humanoid robots. These implementations can be adapted and extended for specific robot platforms and social interaction scenarios. The integration example shows how all these social behaviors can work together in a cohesive interaction system."}),"\n",(0,o.jsx)(n.p,{children:"In practice, social robotics systems need to be extensively tested with human users to validate their naturalness, safety, and effectiveness. The evaluation framework provided gives a starting point for assessing social interaction performance."})]})}function p(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(m,{...e})}):m(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>s});var o=t(6540);const a={},i=o.createContext(a);function r(e){const n=o.useContext(i);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),o.createElement(i.Provider,{value:n},e.children)}}}]);