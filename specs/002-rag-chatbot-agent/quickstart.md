# Quickstart Guide: RAG-Based Chatbot for Educational Content

## Overview
This guide will help you set up and run the RAG-based chatbot that answers questions about book content using vector search and AI models.

## Prerequisites
- Python 3.11+
- Docker and Docker Compose (for running Qdrant)
- API keys for OpenAI and Google Generative AI (for Gemini)

## Setup

### 1. Clone the Repository
```bash
git clone <repository-url>
cd <repository-name>
```

### 2. Prepare Python Environment
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

### 3. Set Up Environment Variables
Create a `.env` file in the root directory:
```env
OPENAI_API_KEY=your_openai_api_key
GOOGLE_API_KEY=your_google_api_key
QDRANT_URL=http://localhost:6333
```

### 4. Run Qdrant Vector Database
```bash
docker-compose up -d
```

### 5. Install Dependencies
```bash
pip install -r requirements.txt
```

## Running the Application

### 1. Start the Backend Server
```bash
cd backend
uvicorn src.api.main:app --reload --port 8000
```

### 2. Index Book Content
Before the chatbot can answer questions, you need to index the book content:
```bash
python -m src.services.rag_service index-book --book-path path/to/book/content
```

### 3. Test the API
```bash
curl -X POST "http://localhost:8000/chat" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What is the main concept discussed in chapter 1?"
  }'
```

## Frontend Integration

The chatbot UI component can be integrated into the textbook frontend:

1. Add the chatbot widget to textbook pages
2. Configure the widget to send queries to your backend API
3. Display responses in context with the textbook content

### Sample Frontend Integration
```jsx
import ChatbotWidget from './components/ChatbotWidget';

// In your textbook page component:
return (
  <div className="textbook-content">
    {/* Your textbook content */}
    <ChatbotWidget 
      apiEndpoint="http://localhost:8000/chat"
      context={currentChapterContent}
    />
  </div>
);
```

## API Usage

### Submit a Query
```bash
curl -X POST "http://localhost:8000/chat" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "Explain quantum computing in simple terms",
    "sessionId": "session-123"
  }'
```

### Create a Session
```bash
curl -X POST "http://localhost:8000/chat/session" \
  -H "Content-Type: application/json" \
  -d '{
    "userId": "user-456"
  }'
```

### End a Session
```bash
curl -X DELETE "http://localhost:8000/chat/session/session-123"
```

## Architecture Overview

```
┌─────────────────┐    ┌──────────────────┐    ┌─────────────────┐
│   Textbook      │    │   FastAPI        │    │   Qdrant        │
│   Frontend      │───▶│   Backend        │───▶│   Vector DB     │
│                 │    │                  │    │                 │
└─────────────────┘    └──────────────────┘    └─────────────────┘
                              │
                              ▼
                       ┌──────────────────┐
                       │   AI Providers   │
                       │ (OpenAI, Gemini) │
                       └──────────────────┘
```

## Configuration
- To switch between OpenAI and Gemini API, modify the AI_PROVIDER environment variable
- Adjust embedding model settings in the vector store configuration
- Configure the context window size for conversation history

## Next Steps
1. Check out the detailed data models in `data-model.md`
2. Review the full API specification in `contracts/chat-api.yaml`
3. Explore the implementation tasks in `tasks.md` (generated by `/sp.tasks`)